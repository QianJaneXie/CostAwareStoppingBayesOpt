{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data...\n",
      "==> No cached data found or cache set to False.\n",
      "==> Reading json data...\n",
      "==> Done.\n"
     ]
    }
   ],
   "source": [
    "from LCBench.api import Benchmark\n",
    "import os\n",
    "\n",
    "os.makedirs(\"LCBench/cached\", exist_ok=True)\n",
    "bench_dir = \"LCBench/cached/six_datasets_lw.json\"\n",
    "bench = Benchmark(bench_dir, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set default tensor type to float64\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_config(config):\n",
    "    # Convert each value to a torch tensor (ensuring float type for calculations)\n",
    "    batch = torch.tensor(config[\"batch_size\"])\n",
    "    lr = torch.tensor(config[\"learning_rate\"])\n",
    "    units = torch.tensor(config[\"max_units\"])\n",
    "    momentum = torch.tensor(config[\"momentum\"])\n",
    "    weight_decay = torch.tensor(config[\"weight_decay\"])\n",
    "    layers = torch.tensor(float(config[\"num_layers\"]))\n",
    "    dropout = torch.tensor(config[\"max_dropout\"])\n",
    "    \n",
    "    # For log-scaled parameters: batch size, learning rate, and max units.\n",
    "    batch_norm = (torch.log(batch) - torch.log(torch.tensor(16.0))) / (torch.log(torch.tensor(512.0)) - torch.log(torch.tensor(16.0)))\n",
    "    lr_norm = (torch.log(lr) - torch.log(torch.tensor(1e-4))) / (torch.log(torch.tensor(1e-1)) - torch.log(torch.tensor(1e-4)))\n",
    "    units_norm = (torch.log(units) - torch.log(torch.tensor(64.0))) / (torch.log(torch.tensor(1024.0)) - torch.log(torch.tensor(64.0)))\n",
    "    \n",
    "    # For linearly scaled parameters.\n",
    "    momentum_norm = (momentum - 0.1) / (0.99 - 0.1)\n",
    "    weight_decay_norm = (weight_decay - 1e-5) / (1e-1 - 1e-5)\n",
    "    layers_norm = (layers - 1) / (4 - 1)\n",
    "    \n",
    "    # Dropout is already between 0 and 1.\n",
    "    dropout_norm = dropout\n",
    "\n",
    "    # Combine into a 7-dimensional tensor.\n",
    "    normalized_vector = torch.stack([\n",
    "        batch_norm, \n",
    "        lr_norm, \n",
    "        momentum_norm, \n",
    "        weight_decay_norm, \n",
    "        layers_norm, \n",
    "        units_norm, \n",
    "        dropout_norm\n",
    "    ])\n",
    "    \n",
    "    return normalized_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = []\n",
    "all_y = []\n",
    "all_c = []\n",
    "dataset_name = \"higgs\"\n",
    "for config_id in bench.data[dataset_name].keys():\n",
    "    config = bench.query(dataset_name, \"config\", config_id)\n",
    "    all_x.append(normalize_config(config))\n",
    "    val_ce = bench.query(dataset_name, \"final_val_cross_entropy\", config_id)\n",
    "    all_y.append(val_ce)\n",
    "    runtime = bench.query(dataset_name, \"time\", config_id)[-1]\n",
    "    all_c.append(runtime)\n",
    "\n",
    "all_x = torch.stack(all_x)\n",
    "all_y = torch.tensor(all_y).unsqueeze(1)\n",
    "all_c = torch.tensor(all_c).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4299, 0.4204, 0.1272,  ..., 0.6667, 0.5487, 0.0259],\n",
       "         [0.9672, 0.6977, 0.0720,  ..., 1.0000, 0.9729, 0.5472],\n",
       "         [0.8919, 0.1077, 0.3272,  ..., 0.0000, 0.8208, 0.3320],\n",
       "         ...,\n",
       "         [0.6750, 0.8598, 0.4454,  ..., 0.6667, 0.4707, 0.3635],\n",
       "         [0.9691, 0.3290, 0.0093,  ..., 0.3333, 0.8684, 0.0437],\n",
       "         [0.3666, 0.9906, 0.2041,  ..., 1.0000, 0.6681, 0.4045]]),\n",
       " tensor([[0.6380],\n",
       "         [0.6931],\n",
       "         [0.7014],\n",
       "         ...,\n",
       "         [0.6090],\n",
       "         [0.6511],\n",
       "         [0.6931]]),\n",
       " tensor([[215.9746],\n",
       "         [876.6520],\n",
       "         [126.5395],\n",
       "         ...,\n",
       "         [186.1553],\n",
       "         [165.5783],\n",
       "         [877.6618]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x, all_y, all_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandora_automl.utils import fit_gp_model\n",
    "import numpy as np\n",
    "import math\n",
    "from botorch.acquisition import LogExpectedImprovement\n",
    "from pandora_automl.acquisition.log_ei_puc import LogExpectedImprovementWithCost\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from pandora_automl.acquisition.lcb import LowerConfidenceBound\n",
    "from pandora_automl.acquisition.gittins import GittinsIndex\n",
    "from pandora_automl.acquisition.stable_gittins import StableGittinsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Initial config id: [232, 1429, 140, 373, 247, 624, 1804, 811, 1959, 619, 1653, 470, 1727, 1453, 1409, 765]\n",
      "Iteration 1:\n",
      "  Selected config_id: 1897\n",
      "  Acquisition value: 0.5932\n",
      "  Objective (final_val_cross_entropy): 0.6171\n",
      "  Cost (time): 186.8864\n",
      "  Current best observed: 0.6185\n",
      "\n",
      "Iteration 2:\n",
      "  Selected config_id: 1691\n",
      "  Acquisition value: 0.5886\n",
      "  Objective (final_val_cross_entropy): 0.6150\n",
      "  Cost (time): 163.1292\n",
      "  Current best observed: 0.6171\n",
      "\n",
      "Iteration 3:\n",
      "  Selected config_id: 1578\n",
      "  Acquisition value: 0.5881\n",
      "  Objective (final_val_cross_entropy): 0.5781\n",
      "  Cost (time): 182.8854\n",
      "  Current best observed: 0.6150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim = 7\n",
    "n_iter = 3\n",
    "maximize = False\n",
    "output_standardize = True\n",
    "acq = \"StablePBGI(1e-6)\"\n",
    "\n",
    "torch.manual_seed(15)\n",
    "init_config_id = torch.randint(low=0, high=2000, size=(2*(dim+1),))\n",
    "config_id_history = init_config_id.tolist()\n",
    "print(f\"  Initial config id: {config_id_history}\")\n",
    "x = all_x[init_config_id]\n",
    "y = all_y[init_config_id]\n",
    "c = all_c[init_config_id]\n",
    "best_y_history = [y.min().item()]\n",
    "best_id_history = [config_id_history[y.argmin().item()]]\n",
    "cost_history = [0]\n",
    "StablePBGI_1e_5_acq_history = [np.nan]\n",
    "# StablePBGI_3e_6_acq_history = [np.nan]\n",
    "StablePBGI_1e_6_acq_history = [np.nan]\n",
    "# StablePBGI_3e_7_acq_history = [np.nan]\n",
    "StablePBGI_1e_7_acq_history = [np.nan]\n",
    "LogEIC_inv_acq_history = [np.nan]\n",
    "LogEIC_exp_acq_history = [np.nan]\n",
    "regret_upper_bound_history = [np.nan]\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # 1. Fit a GP model on the current data.\n",
    "    model = fit_gp_model(X=x, objective_X=y, cost_X=c, unknown_cost=True, output_standardize=output_standardize)\n",
    "    \n",
    "    # 2. Determine the best observed objective value.\n",
    "    best_f = y.min()\n",
    "        \n",
    "    # 3. Define the acquisition function.\n",
    "    StablePBGI_1e_5 = StableGittinsIndex(model=model, maximize=maximize, lmbda=1e-5, unknown_cost=True)\n",
    "    # StablePBGI_3e_6 = StableGittinsIndex(model=model, maximize=maximize, lmbda=3e-6, unknown_cost=True)\n",
    "    StablePBGI_1e_6 = StableGittinsIndex(model=model, maximize=maximize, lmbda=1e-6, unknown_cost=True)\n",
    "    # StablePBGI_3e_7 = StableGittinsIndex(model=model, maximize=maximize, lmbda=3e-7, unknown_cost=True)\n",
    "    StablePBGI_1e_7 = StableGittinsIndex(model=model, maximize=maximize, lmbda=1e-7, unknown_cost=True)\n",
    "    LogEIC_inv = LogExpectedImprovementWithCost(model=model, best_f=best_f, maximize=maximize, unknown_cost=True, inverse_cost=True)\n",
    "    LogEIC_exp = LogExpectedImprovementWithCost(model=model, best_f=best_f, maximize=maximize, unknown_cost=True, inverse_cost=False)\n",
    "    single_outcome_model = fit_gp_model(X=x, objective_X=y, output_standardize=output_standardize)\n",
    "    UCB = UpperConfidenceBound(model=single_outcome_model, maximize=maximize, beta=2 * np.log(dim * ((i + 1) ** 2) * (math.pi ** 2) / (6 * 0.1)) / 5)\n",
    "    LCB = LowerConfidenceBound(model=single_outcome_model, maximize=maximize, beta=2 * np.log(dim * ((i + 1) ** 2) * (math.pi ** 2) / (6 * 0.1)) / 5)\n",
    "\n",
    "    # 4. Evaluate the acquisition function on all candidate x's.\n",
    "    # The unsqueeze operations add extra dimensions if required by your model.\n",
    "\n",
    "    StablePBGI_1e_5_acq = StablePBGI_1e_5.forward(all_x.unsqueeze(1))\n",
    "    StablePBGI_1e_6_acq = StablePBGI_1e_6.forward(all_x.unsqueeze(1))\n",
    "    StablePBGI_1e_7_acq = StablePBGI_1e_7.forward(all_x.unsqueeze(1))\n",
    "    LogEIC_inv_acq = LogEIC_inv.forward(all_x.unsqueeze(1))\n",
    "    LogEIC_exp_acq = LogEIC_exp.forward(all_x.unsqueeze(1))\n",
    "    UCB_acq = UCB.forward(all_x.unsqueeze(1))\n",
    "    LCB_acq = LCB.forward(all_x.unsqueeze(1))\n",
    "\n",
    "    # 5. Record information for stopping.\n",
    "    num_configs = 2000\n",
    "    all_ids = torch.arange(num_configs)\n",
    "    mask = torch.ones(num_configs, dtype=torch.bool)\n",
    "    mask[config_id_history] = False\n",
    "\n",
    "    StablePBGI_1e_5_acq_history.append(torch.min(StablePBGI_1e_5_acq[mask]).item())\n",
    "    # StablePBGI_3e_6_acq_history.append(torch.min(StablePBGI_3e_6_acq[mask]).item())\n",
    "    StablePBGI_1e_6_acq_history.append(torch.min(StablePBGI_1e_6_acq[mask]).item())\n",
    "    # StablePBGI_3e_7_acq_history.append(torch.min(StablePBGI_3e_7_acq[mask]).item())\n",
    "    StablePBGI_1e_7_acq_history.append(torch.min(StablePBGI_1e_7_acq[mask]).item())\n",
    "    LogEIC_inv_acq_history.append(torch.max(LogEIC_inv_acq[mask]).item())\n",
    "    LogEIC_exp_acq_history.append(torch.max(LogEIC_exp_acq[mask]).item())\n",
    "    regret_upper_bound_history.append(torch.min(UCB_acq).item() - torch.min(LCB_acq).item())\n",
    "\n",
    "    # 6. Select the candidate with the optimal acquisition value.\n",
    "    candidate_ids = all_ids[mask]\n",
    "\n",
    "    if acq == \"StablePBGI(1e-5)\":\n",
    "        candidate_acqs = StablePBGI_1e_5_acq[mask]\n",
    "        new_config_id = candidate_ids[torch.argmin(candidate_acqs)]\n",
    "        new_config_acq = torch.min(candidate_acqs)\n",
    "    # if acq == \"StablePBGI(3e-6)\":\n",
    "    #     candidate_acqs = StablePBGI_3e_6_acq[mask]\n",
    "    #     new_config_id = candidate_ids[torch.argmin(candidate_acqs)]\n",
    "    #     new_config_acq = torch.min(candidate_acqs)\n",
    "    if acq == \"StablePBGI(1e-6)\":\n",
    "        candidate_acqs = StablePBGI_1e_6_acq[mask]\n",
    "        new_config_id = candidate_ids[torch.argmin(candidate_acqs)]\n",
    "        new_config_acq = torch.min(candidate_acqs)\n",
    "    # if acq == \"StablePBGI(3e-7)\":\n",
    "    #     candidate_acqs = StablePBGI_3e_7_acq[mask]\n",
    "    #     new_config_id = candidate_ids[torch.argmin(candidate_acqs)]\n",
    "    #     new_config_acq = torch.min(candidate_acqs)\n",
    "    if acq == \"StablePBGI(1e-7)\":\n",
    "        candidate_acqs = StablePBGI_1e_7_acq[mask]\n",
    "        new_config_id = candidate_ids[torch.argmin(candidate_acqs)]\n",
    "        new_config_acq = torch.min(candidate_acqs)\n",
    "    if acq == \"LogEIC-inv\":\n",
    "        candidate_acqs = LogEIC_inv_acq[mask]\n",
    "        new_config_id = candidate_ids[torch.argmax(candidate_acqs)]\n",
    "        new_config_acq = torch.max(candidate_acqs)\n",
    "    if acq == \"LogEIC-exp\":\n",
    "        candidate_acqs = LogEIC_exp_acq[mask]\n",
    "        new_config_id = candidate_ids[torch.argmax(candidate_acqs)]\n",
    "        new_config_acq = torch.max(candidate_acqs)\n",
    "    if acq == \"LCB\":\n",
    "        candidate_acqs = LCB_acq[mask]\n",
    "        new_config_id = candidate_ids[torch.argmin(candidate_acqs)]\n",
    "        new_config_acq = torch.min(candidate_acqs)\n",
    "\n",
    "    new_config_x = all_x[new_config_id]\n",
    "    \n",
    "    # 7. Query the objective for the new configuration.\n",
    "    new_config_y = all_y[new_config_id]\n",
    "    new_config_c = all_c[new_config_id]\n",
    "    \n",
    "    # 8. Append the new data to our training set.\n",
    "    x = torch.cat([x, new_config_x.unsqueeze(0)], dim=0)\n",
    "    y = torch.cat([y, new_config_y.unsqueeze(0)], dim=0)\n",
    "    c = torch.cat([c, new_config_c.unsqueeze(0)], dim=0)\n",
    "    config_id_history.append(new_config_id.item())\n",
    "    best_y_history.append(best_f.item())\n",
    "    best_id_history.append(config_id_history[y.argmin().item()])\n",
    "    cost_history.append(new_config_c.item())\n",
    "\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    print(f\"  Selected config_id: {new_config_id}\")\n",
    "    print(f\"  Acquisition value: {new_config_acq.item():.4f}\")\n",
    "    print(f\"  Objective (final_val_cross_entropy): {new_config_y.item():.4f}\")\n",
    "    print(f\"  Cost (time): {new_config_c.item():.4f}\")\n",
    "    print(f\"  Current best observed: {best_f.item():.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "best_y_history.append(y.min().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
